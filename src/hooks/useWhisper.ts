import { useState, useEffect, useRef, useCallback } from 'react';

// Interfaces (WhisperMessage, WhisperEventData)ëŠ” ë³€ê²½ ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
interface WhisperMessage {
  id: number;
  type: 'load' | 'transcribe';
  payload: any;
}

interface WhisperEventData {
  id: number;
  type: 'load' | 'transcribe' | 'error';
  payload: {
    status?: 'progress' | 'complete' | 'error';
    progress?: number;
    message?: string;
    error?: string;
    data?: {
      text: string;
    };
  };
}


export const useWhisper = () => {
  const [isModelLoading, setIsModelLoading] = useState(false);
  const [isReady, setIsReady] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [error, setError] = useState<string | null>(null);
  const [loadingProgress, setLoadingProgress] = useState<number | null>(null);

  const workerRef = useRef<Worker | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const messageIdRef = useRef(0);
  const audioContextRef = useRef<AudioContext | null>(null);

  useEffect(() => {
    const whisperWorker = new Worker('/whisper/whisper.worker.js', {
      type: 'module'
    });
    console.log('Whisper Workerê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.');

    whisperWorker.onmessage = (event: MessageEvent<WhisperEventData | any>) => {
      const { type, payload } = event.data;

      switch (type) {
        case 'load':
          handleLoadMessage(payload);
          break;
        case 'transcribe':
          handleTranscribeMessage(payload);
          break;
        case 'error':
          handleErrorMessage(payload);
          break;
        default:
          break;
      }
    };

    whisperWorker.onerror = (err) => {
      console.error('Worker error:', err);
      setError(`Worker error: ${err.message}`);
      setIsModelLoading(false);
      setLoadingProgress(null);
    };

    workerRef.current = whisperWorker;
    loadModel('ggml-base.bin');

    return () => {
      whisperWorker.terminate();
      if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
        mediaRecorderRef.current.stop();
      }
      audioContextRef.current?.close();
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  const handleLoadMessage = (payload: WhisperEventData['payload']) => {
    if (payload.status === 'complete') {
      setIsModelLoading(false);
      setIsReady(true);
      setLoadingProgress(null);
      console.log('Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ.');
    } else if (payload.status === 'progress') {
      setLoadingProgress(payload.progress || 0);
    } else {
      setIsModelLoading(false);
      setError(payload.error || 'ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
      setLoadingProgress(null);
    }
  };

  const handleTranscribeMessage = (payload: WhisperEventData['payload']) => {
    if (payload.status === 'complete' && payload.data) {
      const transcribedText = payload.data.text.trim();
      if (transcribedText && transcribedText !== '[BLANK_AUDIO]') {
        setTranscript((prev) => prev + transcribedText + ' ');
        console.log('ğŸ¤ ìŒì„± ë³€í™˜ ê²°ê³¼ ì¶”ê°€:', transcribedText);
      }
    } else if (payload.status === 'error') {
      console.error('ìŒì„± ë³€í™˜ ì˜¤ë¥˜:', payload.error);
      // Worker ë‚´ë¶€ì˜ ë³€í™˜ ì‹¤íŒ¨ëŠ” ì—ëŸ¬ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
      setError(payload.error || 'ìŒì„± ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
    }
  };

  const handleErrorMessage = (payload: WhisperEventData['payload']) => {
    setError(payload.error || 'ì•Œ ìˆ˜ ì—†ëŠ” Worker ì˜¤ë¥˜');
    setIsModelLoading(false);
    setLoadingProgress(null);
  };

  const postMessageToWorker = (type: 'load' | 'transcribe', payload: any) => {
    if (!workerRef.current) return;
    const id = messageIdRef.current++;
    const message: WhisperMessage = { id, type, payload };
    workerRef.current.postMessage(message);
  };

  const loadModel = useCallback((model = 'ggml-base.bin') => {
    setIsModelLoading(true);
    setIsReady(false);
    setError(null);
    setLoadingProgress(0);
    postMessageToWorker('load', { model });
  }, []);

  const startRecording = useCallback(async () => {
    if (!isReady || isRecording) return;

    setTranscript('');
    setError(null);

    try {
      if (!audioContextRef.current || audioContextRef.current.state === 'closed') {
        audioContextRef.current = new AudioContext({ sampleRate: 16000 });
      }
      // ë¸Œë¼ìš°ì €ì— ì˜í•´ ì •ì§€ëœ AudioContextë¥¼ ì¬í™œì„±í™”í•©ë‹ˆë‹¤.
      if (audioContextRef.current.state === 'suspended') {
        await audioContextRef.current.resume();
      }

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });

      mediaRecorderRef.current = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      });

      mediaRecorderRef.current.ondataavailable = (event) => {
        // [ìˆ˜ì •ë¨] ìµœì†Œ Blob í¬ê¸°ë¥¼ í™•ì¸í•˜ì—¬ ë¹ˆ ì²­í¬ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.
        // 100ë°”ì´íŠ¸ëŠ” í—¤ë”ë§Œ ìˆê³  ë°ì´í„°ëŠ” ì—†ëŠ” Blobì„ ê±°ë¥´ê¸° ìœ„í•œ ì„ì˜ì˜ ì„ê³„ê°’ì…ë‹ˆë‹¤.
        if (event.data.size > 100) {
          transcribe(event.data);
        }
      };

      mediaRecorderRef.current.onstop = () => {
        console.log('ë…¹ìŒì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.');
        stream.getTracks().forEach(track => track.stop());
      };

      mediaRecorderRef.current.start(1000);
      setIsRecording(true);

    } catch (err) {
      console.error('ë§ˆì´í¬ ì ‘ê·¼ ì˜¤ë¥˜:', err);
      setError('ë§ˆì´í¬ ì ‘ê·¼ì´ ê±°ë¶€ë˜ì—ˆê±°ë‚˜ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    }
  }, [isReady, isRecording]);

  const stopRecording = useCallback(() => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
    }
  }, [isRecording]);

  const transcribe = async (audioBlob: Blob) => {
    if (!workerRef.current || !isReady || !audioContextRef.current) return;

    try {
      const arrayBuffer = await audioBlob.arrayBuffer();
      const audioBuffer = await audioContextRef.current.decodeAudioData(arrayBuffer);
      const audioData = audioBuffer.getChannelData(0);

      // ë¦¬ìƒ˜í”Œë§ ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ (ê²¬ê³ ì„±)
      const targetSampleRate = 16000;
      let resampledAudio = audioData;
      if (audioBuffer.sampleRate !== targetSampleRate) {
        const resampleRatio = audioBuffer.sampleRate / targetSampleRate;
        const targetLength = Math.floor(audioData.length / resampleRatio);
        resampledAudio = new Float32Array(targetLength);
        for (let i = 0; i < targetLength; i++) {
          resampledAudio[i] = audioData[Math.floor(i * resampleRatio)];
        }
      }

      postMessageToWorker('transcribe', {
        audio: {
          sampling_rate: targetSampleRate,
          data: resampledAudio,
        },
        language: 'ko',
        translate: false,
      });

    } catch (err) {
      // [ìˆ˜ì •ë¨] EncodingErrorë¥¼ ì •ìƒì ì¸ ê²½ê³ ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
      if (err instanceof DOMException && err.name === 'EncodingError') {
        console.warn('ë””ì½”ë”©í•  ìˆ˜ ì—†ëŠ” ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤. (ì •ìƒ ë™ì‘)');
      } else {
        // ê·¸ ì™¸ì˜ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ëŠ” ìƒíƒœë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
        console.error('ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜:', err);
        setError('ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
      }
    }
  };

  return {
    isModelLoading,
    isReady,
    isRecording,
    transcript,
    error,
    loadingProgress,
    startRecording,
    stopRecording,
    loadModel
  };
};